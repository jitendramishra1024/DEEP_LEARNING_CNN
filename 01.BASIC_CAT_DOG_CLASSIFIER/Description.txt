We are going  to test an provided image is dog or cat 
i.e binary classification using Convolution neural network
dataset :
training_set: 4000 cat image 4000 dog image 
test_set: 1000 cat image 1000 dog image 
single_prediction :2 images cat_or_dog 

STEPS :
1.upload dataset.zip to google drive Colab Notebooks 
2.Create a new python notebook in google colab rename as CAT_DOG_CLASSIFIER and save this will be saved in Colab Notebooks 
3.To make google cab access data from google drive 
  from google.colab import drive 
  drive.mount('/content/drive')
4. run CAT_DOG_CLASSIFIER.ipynb 

#EXTRA :
Data augmentation to avoid overfitting 
https://keras.io/api/preprocessing/image/
Solution  use ImageDatagenerator Class from Keras Generate batches of tensor image data with real-time data augmentation.
2 ways to use it
a.flow 
b.flow_from_directory

flow_from_directory to be used when we have data in below format 
train folder->class1 folder
            ->class2 folder 
test folder ->class1 folder
            ->class2 folder 

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)



model.fit and model.fit_generator 
.fit is used when the entire training dataset can fit into the memory and no data augmentation is applied.
.fit_generator is used when either we have a huge dataset to fit into our memory or when data augmentation needs to be applied.
important parameters :
history =CNN_Classifier.fit_generator(
        training_set,
        steps_per_epoch=200,
        epochs=20,
        validation_data=test_set,
        validation_steps=100, verbose=1)
training_set:training data 
-> steps_per_epoch : it specifies the total number of steps taken from the generator
 as soon as one epoch is finished and next epoch has started. We can calculate the value
of steps_per_epoch as the total number of samples in your dataset divided by the batch size.
-> Epochs : an integer and number of epochs we want to train our model for.
-> Verbose : specifies verbosity mode(0 = silent, 1= progress bar, 2 = one line per epoch).
-> validation_data is the test data : test_set here 
-> validation_steps :only if the validation_data is a generator then only this argument
can be used. It specifies the total number of steps taken from the generator before it is 
stopped at every epoch and its value is calculated as the total number of training data points
in your dataset divided by the batch size.

